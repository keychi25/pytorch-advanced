{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('pytorch_p36': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b32c976f4cff9f05e2fcc2f59241d4b3454b6c3c855c347bb189d25082248dc6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# BERTモデルの実装"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "source": [
    "## Bertの実装\n",
    "### BERT_Baseのネットワークの設定ファイルの読み込み"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import json\n",
    "\n",
    "config_file = \"./weights/bert_config.json\"\n",
    "\n",
    "# ファイルを開き、JSONとして読み込む\n",
    "json_file = open(config_file, 'r')\n",
    "config = json.load(json_file)\n",
    "\n",
    "# 出力確認\n",
    "config"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'attention_probs_dropout_prob': 0.1,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'hidden_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'max_position_embeddings': 512,\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'type_vocab_size': 2,\n",
       " 'vocab_size': 30522}"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "config['hidden_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from attrdict import AttrDict\n",
    "\n",
    "config = AttrDict(config)\n",
    "config.hidden_size"
   ]
  },
  {
   "source": [
    "### BERT用にLayerNormalization層を定義"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "    \"\"\"LayerNormalization層 \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))  # weightのこと\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))  # biasのこと\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "source": [
    "### Embeddingsモジュールの実装"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"文章の単語ID列と、1文目か2文目かの情報を、埋め込みベクトルに変換する\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "\n",
    "        # 3つのベクトル表現の埋め込み\n",
    "\n",
    "        # Token Embedding：単語IDを単語ベクトルに変換、\n",
    "        # vocab_size = 30522でBERTの学習済みモデルで使用したボキャブラリーの量\n",
    "        # hidden_size = 768 で特徴量ベクトルの長さは768\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        # （注釈）padding_idx=0はidx=0の単語のベクトルは0にする。BERTのボキャブラリーのidx=0が[PAD]である。\n",
    "\n",
    "        # Transformer Positional Embedding：位置情報テンソルをベクトルに変換\n",
    "        # Transformerの場合はsin、cosからなる固定値だったが、BERTは学習させる\n",
    "        # max_position_embeddings = 512　で文の長さは512単語\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size)\n",
    "\n",
    "        # Sentence Embedding：文章の1文目、2文目の情報をベクトルに変換\n",
    "        # type_vocab_size = 2\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # 作成したLayerNormalization層\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        # Dropout　'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        '''\n",
    "        input_ids： [batch_size, seq_len]の文章の単語IDの羅列\n",
    "        token_type_ids：[batch_size, seq_len]の各単語が1文目なのか、2文目なのかを示すid\n",
    "        '''\n",
    "\n",
    "        # 1. Token Embeddings\n",
    "        # 単語IDを単語ベクトルに変換\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "\n",
    "        # 2. Sentence Embedding\n",
    "        # token_type_idsがない場合は文章の全単語を1文目として、0にする\n",
    "        # そこで、input_idsと同じサイズのゼロテンソルを作成\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # 3. Transformer Positional Embedding：\n",
    "        # [0, 1, 2 ・・・]と文章の長さだけ、数字が1つずつ昇順に入った\n",
    "        # [batch_size, seq_len]のテンソルposition_idsを作成\n",
    "        # position_idsを入力して、position_embeddings層から768次元のテンソルを取り出す\n",
    "        seq_length = input_ids.size(1)  # 文章の長さ\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        # 3つの埋め込みテンソルを足し合わせる [batch_size, seq_len, hidden_size]\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "\n",
    "        # LayerNormalizationとDropoutを実行\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "source": [
    "### BertLayerモジュール"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    '''BERTのBertLayerモジュールです。Transformerになります'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "\n",
    "        # Self-Attention部分\n",
    "        self.attention = BertAttention(config)\n",
    "\n",
    "        # Self-Attentionの出力を処理する全結合層\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "\n",
    "        # Self-Attentionによる特徴量とBertLayerへの元の入力を足し算する層\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states：Embedderモジュールの出力テンソル[batch_size, seq_len, hidden_size]\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキング\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "            attention_output, attention_probs = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output, attention_probs\n",
    "\n",
    "        elif attention_show_flg == False:\n",
    "            attention_output = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "\n",
    "            return layer_output  # [batch_size, seq_length, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    '''BertLayerモジュールのSelf-Attention部分です'''\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.selfattn = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        input_tensor：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "            self_output, attention_probs = self.selfattn(input_tensor, attention_mask, attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            self_output = self.selfattn(input_tensor, attention_mask, attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    '''BertAttentionのSelf-Attentionです'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        # num_attention_heads': 12\n",
    "\n",
    "        self.attention_head_size = int(\n",
    "            config.hidden_size / config.num_attention_heads)  # 768/12=64\n",
    "        self.all_head_size = self.num_attention_heads * \\\n",
    "            self.attention_head_size  # = 'hidden_size': 768\n",
    "\n",
    "        # Self-Attentionの特徴量を作成する全結合層\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        '''multi-head Attention用にテンソルの形を変換する\n",
    "        [batch_size, seq_len, hidden] → [batch_size, 12, seq_len, hidden/12] \n",
    "        '''\n",
    "        new_x_shape = x.size()[\n",
    "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "        # 入力を全結合層で特徴量変換（注意、multi-head Attentionの全部をまとめて変換しています）\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # multi-head Attention用にテンソルの形を変換\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # 特徴量同士を掛け算して似ている度合をAttention_scoresとして求める\n",
    "        attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / \\\n",
    "            math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # マスクがある部分にはマスクをかけます\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        # （備考）\n",
    "        # マスクが掛け算でなく足し算なのが直感的でないですが、このあとSoftmaxで正規化するので、\n",
    "        # マスクされた部分は-infにしたいです。 attention_maskには、0か-infが\n",
    "        # もともと入っているので足し算にしています。\n",
    "\n",
    "        # Attentionを正規化する\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # ドロップアウトします\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Attention Mapを掛け算します\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # multi-head Attentionのテンソルの形をもとに戻す\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[\n",
    "            :-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        # attention_showのときは、attention_probsもリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return context_layer, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    '''BertSelfAttentionの出力を処理する全結合層です'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states：BertSelfAttentionの出力テンソル\n",
    "        input_tensor：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    '''Gaussian Error Linear Unitという活性化関数です。\n",
    "    LeLUが0でカクっと不連続なので、そこを連続になるように滑らかにした形のLeLUです。\n",
    "    '''\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    '''BERTのTransformerBlockモジュールのFeedForwardです'''\n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        \n",
    "        # 全結合層：'hidden_size': 768、'intermediate_size': 3072\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        \n",
    "        # 活性化関数gelu\n",
    "        self.intermediate_act_fn = gelu\n",
    "            \n",
    "    def forward(self, hidden_states):\n",
    "        '''\n",
    "        hidden_states： BertAttentionの出力テンソル\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)  # GELUによる活性化\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    '''BERTのTransformerBlockモジュールのFeedForwardです'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "\n",
    "        # 全結合層：'intermediate_size': 3072、'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        # 'hidden_dropout_prob': 0.1\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        '''\n",
    "        hidden_states： BertIntermediateの出力テンソル\n",
    "        input_tensor：BertAttentionの出力テンソル\n",
    "        '''\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        '''BertLayerモジュールの繰り返し部分モジュールの繰り返し部分です'''\n",
    "        super(BertEncoder, self).__init__()\n",
    "\n",
    "        # config.num_hidden_layers の値、すなわち12 個のBertLayerモジュールを作ります\n",
    "        self.layer = nn.ModuleList([BertLayer(config)\n",
    "                                    for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, attention_show_flg=False):\n",
    "        '''\n",
    "        hidden_states：Embeddingsモジュールの出力\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：返り値を全TransformerBlockモジュールの出力にするか、\n",
    "        それとも、最終層だけにするかのフラグ。\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # 返り値として使うリスト\n",
    "        all_encoder_layers = []\n",
    "\n",
    "        # BertLayerモジュールの処理を繰り返す\n",
    "        for layer_module in self.layer:\n",
    "\n",
    "            if attention_show_flg == True:\n",
    "                '''attention_showのときは、attention_probsもリターンする'''\n",
    "                hidden_states, attention_probs = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_flg)\n",
    "            elif attention_show_flg == False:\n",
    "                hidden_states = layer_module(\n",
    "                    hidden_states, attention_mask, attention_show_flg)\n",
    "\n",
    "            # 返り値にBertLayerから出力された特徴量を12層分、すべて使用する場合の処理\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "\n",
    "        # 返り値に最後のBertLayerから出力された特徴量だけを使う場合の処理\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "\n",
    "        # attention_showのときは、attention_probs（最後の12段目）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return all_encoder_layers, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    '''入力文章の1単語目[cls]の特徴量を変換して保持するためのモジュール'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "\n",
    "        # 全結合層、'hidden_size': 768\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 1単語目の特徴量を取得\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "\n",
    "        # 全結合層で特徴量変換\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        # 活性化関数Tanhを計算\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "入力の単語ID列のテンソルサイズ： torch.Size([2, 5])\n",
      "入力のマスクのテンソルサイズ： torch.Size([2, 5])\n",
      "入力の文章IDのテンソルサイズ： torch.Size([2, 5])\n",
      "拡張したマスクのテンソルサイズ： torch.Size([2, 1, 1, 5])\n",
      "BertEmbeddingsの出力テンソルサイズ： torch.Size([2, 5, 768])\n",
      "BertEncoderの最終層の出力テンソルサイズ： torch.Size([2, 5, 768])\n",
      "BertPoolerの出力テンソルサイズ： torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 入力の単語ID列、batch_sizeは2つ\n",
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print(\"入力の単語ID列のテンソルサイズ：\", input_ids.shape)\n",
    "\n",
    "# マスク\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "print(\"入力のマスクのテンソルサイズ：\", attention_mask.shape)\n",
    "\n",
    "# 文章のID。2つのミニバッチそれぞれについて、0が1文目、1が2文目を示す\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print(\"入力の文章IDのテンソルサイズ：\", token_type_ids.shape)\n",
    "\n",
    "\n",
    "# BERTの各モジュールを用意\n",
    "embeddings = BertEmbeddings(config)\n",
    "encoder = BertEncoder(config)\n",
    "pooler = BertPooler(config)\n",
    "\n",
    "# マスクの変形　[batch_size, 1, 1, seq_length]にする\n",
    "# Attentionをかけない部分はマイナス無限にしたいので、代わりに-10000をかけ算しています\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print(\"拡張したマスクのテンソルサイズ：\", extended_attention_mask.shape)\n",
    "\n",
    "# 順伝搬する\n",
    "out1 = embeddings(input_ids, token_type_ids)\n",
    "print(\"BertEmbeddingsの出力テンソルサイズ：\", out1.shape)\n",
    "\n",
    "out2 = encoder(out1, extended_attention_mask)\n",
    "# out2は、[minibatch, seq_length, embedding_dim]が12個のリスト\n",
    "print(\"BertEncoderの最終層の出力テンソルサイズ：\", out2[0].shape)\n",
    "\n",
    "out3 = pooler(out2[-1])  # out2は12層の特徴量のリストになっているので一番最後を使用\n",
    "print(\"BertPoolerの出力テンソルサイズ：\", out3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    '''モジュールを全部つなげたBERTモデル'''\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        # 3つのモジュールを作成\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # Attentionのマスクと文の1文目、2文目のidが無ければ作成する\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        # マスクの変形　[minibatch, 1, 1, seq_length]にする\n",
    "        # 後ほどmulti-head Attentionで使用できる形にしたいので\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # マスクは0、1だがソフトマックスを計算したときにマスクになるように、0と-infにする\n",
    "        # -infの代わりに-10000にしておく\n",
    "        extended_attention_mask = extended_attention_mask.to(\n",
    "            dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # 順伝搬させる\n",
    "        # BertEmbeddinsモジュール\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "\n",
    "        # BertLayerモジュール（Transformer）を繰り返すBertEncoderモジュール\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "\n",
    "            encoded_layers, attention_probs = self.encoder(embedding_output,\n",
    "                                                           extended_attention_mask,\n",
    "                                                           output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        elif attention_show_flg == False:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        # BertPoolerモジュール\n",
    "        # encoderの一番最後のBertLayerから出力された特徴量を使う\n",
    "        pooled_output = self.pooler(encoded_layers[-1])\n",
    "\n",
    "        # output_all_encoded_layersがFalseの場合はリストではなく、テンソルを返す\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "\n",
    "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return encoded_layers, pooled_output, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return encoded_layers, pooled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "encoded_layersのテンソルサイズ： torch.Size([2, 5, 768])\npooled_outputのテンソルサイズ： torch.Size([2, 768])\nattention_probsのテンソルサイズ： torch.Size([2, 12, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# BERTモデルを作る\n",
    "net = BertModel(config)\n",
    "\n",
    "# 順伝搬させる\n",
    "encoded_layers, pooled_output, attention_probs = net(\n",
    "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "print(\"encoded_layersのテンソルサイズ：\", encoded_layers.shape)\n",
    "print(\"pooled_outputのテンソルサイズ：\", pooled_output.shape)\n",
    "print(\"attention_probsのテンソルサイズ：\", attention_probs.shape)"
   ]
  },
  {
   "source": [
    "## BERTを用いたbank（銀行）とbank（土手）の単語ベクトル表現の比較"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"./weights/pytorch_model.bin\"\n",
    "loaded_state_dict = torch.load(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BertModel(config)\n",
    "net.eval()\n",
    "\n",
    "# 現在のネットワークモデルのパラメータ名\n",
    "param_names = []  # パラメータの名前を格納していく\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    param_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# 現在のネットワークの情報をコピーして新たなstate_dictを作成\n",
    "new_state_dict = net.state_dict().copy()\n",
    "\n",
    "# 新たなstate_dictに学習済みの値を代入\n",
    "for index, (key_name, value) in enumerate(loaded_state_dict.items()):\n",
    "    name = param_names[index]  # 現在のネットワークでのパラメータ名を取得\n",
    "    new_state_dict[name] = value  # 値を入れる\n",
    "\n",
    "    # 現在のネットワークのパラメータを全部ロードしたら終える\n",
    "    if index+1 >= len(param_names):\n",
    "        break\n",
    "\n",
    "# 新たなstate_dictを実装したBERTモデルに与える\n",
    "net.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"text形式のvocabファイルの内容を辞書に格納します\"\"\"\n",
    "    vocab = collections.OrderedDict()  # (単語, id)の順番の辞書変数\n",
    "    ids_to_tokens = collections.OrderedDict()  # (id, 単語)の順番の辞書変数\n",
    "    index = 0\n",
    "\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "\n",
    "            # 格納\n",
    "            vocab[token] = index\n",
    "            ids_to_tokens[index] = token\n",
    "            index += 1\n",
    "\n",
    "    return vocab, ids_to_tokens\n",
    "\n",
    "\n",
    "# 実行\n",
    "vocab_file = \"./vocab/bert-base-uncased-vocab.txt\"\n",
    "vocab, ids_to_tokens = load_vocab(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       " 428),\n",
       "             ('[unused424]', 429),\n",
       "             ('[unused425]', 430),\n",
       "             ('[unused426]', 431),\n",
       "             ('[unused427]', 432),\n",
       "             ('[unused428]', 433),\n",
       "             ('[unused429]', 434),\n",
       "             ('[unused430]', 435),\n",
       "             ('[unused431]', 436),\n",
       "             ('[unused432]', 437),\n",
       "             ('[unused433]', 438),\n",
       "             ('[unused434]', 439),\n",
       "             ('[unused435]', 440),\n",
       "             ('[unused436]', 441),\n",
       "             ('[unused437]', 442),\n",
       "             ('[unused438]', 443),\n",
       "             ('[unused439]', 444),\n",
       "             ('[unused440]', 445),\n",
       "             ('[unused441]', 446),\n",
       "             ('[unused442]', 447),\n",
       "             ('[unused443]', 448),\n",
       "             ('[unused444]', 449),\n",
       "             ('[unused445]', 450),\n",
       "             ('[unused446]', 451),\n",
       "             ('[unused447]', 452),\n",
       "             ('[unused448]', 453),\n",
       "             ('[unused449]', 454),\n",
       "             ('[unused450]', 455),\n",
       "             ('[unused451]', 456),\n",
       "             ('[unused452]', 457),\n",
       "             ('[unused453]', 458),\n",
       "             ('[unused454]', 459),\n",
       "             ('[unused455]', 460),\n",
       "             ('[unused456]', 461),\n",
       "             ('[unused457]', 462),\n",
       "             ('[unused458]', 463),\n",
       "             ('[unused459]', 464),\n",
       "             ('[unused460]', 465),\n",
       "             ('[unused461]', 466),\n",
       "             ('[unused462]', 467),\n",
       "             ('[unused463]', 468),\n",
       "             ('[unused464]', 469),\n",
       "             ('[unused465]', 470),\n",
       "             ('[unused466]', 471),\n",
       "             ('[unused467]', 472),\n",
       "             ('[unused468]', 473),\n",
       "             ('[unused469]', 474),\n",
       "             ('[unused470]', 475),\n",
       "             ('[unused471]', 476),\n",
       "             ('[unused472]', 477),\n",
       "             ('[unused473]', 478),\n",
       "             ('[unused474]', 479),\n",
       "             ('[unused475]', 480),\n",
       "             ('[unused476]', 481),\n",
       "             ('[unused477]', 482),\n",
       "             ('[unused478]', 483),\n",
       "             ('[unused479]', 484),\n",
       "             ('[unused480]', 485),\n",
       "             ('[unused481]', 486),\n",
       "             ('[unused482]', 487),\n",
       "             ('[unused483]', 488),\n",
       "             ('[unused484]', 489),\n",
       "             ('[unused485]', 490),\n",
       "             ('[unused486]', 491),\n",
       "             ('[unused487]', 492),\n",
       "             ('[unused488]', 493),\n",
       "             ('[unused489]', 494),\n",
       "             ('[unused490]', 495),\n",
       "             ('[unused491]', 496),\n",
       "             ('[unused492]', 497),\n",
       "             ('[unused493]', 498),\n",
       "             ('[unused494]', 499),\n",
       "             ('[unused495]', 500),\n",
       "             ('[unused496]', 501),\n",
       "             ('[unused497]', 502),\n",
       "             ('[unused498]', 503),\n",
       "             ('[unused499]', 504),\n",
       "             ('[unused500]', 505),\n",
       "             ('[unused501]', 506),\n",
       "             ('[unused502]', 507),\n",
       "             ('[unused503]', 508),\n",
       "             ('[unused504]', 509),\n",
       "             ('[unused505]', 510),\n",
       "             ('[unused506]', 511),\n",
       "             ('[unused507]', 512),\n",
       "             ('[unused508]', 513),\n",
       "             ('[unused509]', 514),\n",
       "             ('[unused510]', 515),\n",
       "             ('[unused511]', 516),\n",
       "             ('[unused512]', 517),\n",
       "             ('[unused513]', 518),\n",
       "             ('[unused514]', 519),\n",
       "             ('[unused515]', 520),\n",
       "             ('[unused516]', 521),\n",
       "             ('[unused517]', 522),\n",
       "             ('[unused518]', 523),\n",
       "             ('[unused519]', 524),\n",
       "             ('[unused520]', 525),\n",
       "             ('[unused521]', 526),\n",
       "             ('[unused522]', 527),\n",
       "             ('[unused523]', 528),\n",
       "             ('[unused524]', 529),\n",
       "             ('[unused525]', 530),\n",
       "             ('[unused526]', 531),\n",
       "             ('[unused527]', 532),\n",
       "             ('[unused528]', 533),\n",
       "             ('[unused529]', 534),\n",
       "             ('[unused530]', 535),\n",
       "             ('[unused531]', 536),\n",
       "             ('[unused532]', 537),\n",
       "             ('[unused533]', 538),\n",
       "             ('[unused534]', 539),\n",
       "             ('[unused535]', 540),\n",
       "             ('[unused536]', 541),\n",
       "             ('[unused537]', 542),\n",
       "             ('[unused538]', 543),\n",
       "             ('[unused539]', 544),\n",
       "             ('[unused540]', 545),\n",
       "             ('[unused541]', 546),\n",
       "             ('[unused542]', 547),\n",
       "             ('[unused543]', 548),\n",
       "             ('[unused544]', 549),\n",
       "             ('[unused545]', 550),\n",
       "             ('[unused546]', 551),\n",
       "             ('[unused547]', 552),\n",
       "             ('[unused548]', 553),\n",
       "             ('[unused549]', 554),\n",
       "             ('[unused550]', 555),\n",
       "             ('[unused551]', 556),\n",
       "             ('[unused552]', 557),\n",
       "             ('[unused553]', 558),\n",
       "             ('[unused554]', 559),\n",
       "             ('[unused555]', 560),\n",
       "             ('[unused556]', 561),\n",
       "             ('[unused557]', 562),\n",
       "             ('[unused558]', 563),\n",
       "             ('[unused559]', 564),\n",
       "             ('[unused560]', 565),\n",
       "             ('[unused561]', 566),\n",
       "             ('[unused562]', 567),\n",
       "             ('[unused563]', 568),\n",
       "             ('[unused564]', 569),\n",
       "             ('[unused565]', 570),\n",
       "             ('[unused566]', 571),\n",
       "             ('[unused567]', 572),\n",
       "             ('[unused568]', 573),\n",
       "             ('[unused569]', 574),\n",
       "             ('[unused570]', 575),\n",
       "             ('[unused571]', 576),\n",
       "             ('[unused572]', 577),\n",
       "             ('[unused573]', 578),\n",
       "             ('[unused574]', 579),\n",
       "             ('[unused575]', 580),\n",
       "             ('[unused576]', 581),\n",
       "             ('[unused577]', 582),\n",
       "             ('[unused578]', 583),\n",
       "             ('[unused579]', 584),\n",
       "             ('[unused580]', 585),\n",
       "             ('[unused581]', 586),\n",
       "             ('[unused582]', 587),\n",
       "             ('[unused583]', 588),\n",
       "             ('[unused584]', 589),\n",
       "             ('[unused585]', 590),\n",
       "             ('[unused586]', 591),\n",
       "             ('[unused587]', 592),\n",
       "             ('[unused588]', 593),\n",
       "             ('[unused589]', 594),\n",
       "             ('[unused590]', 595),\n",
       "             ('[unused591]', 596),\n",
       "             ('[unused592]', 597),\n",
       "             ('[unused593]', 598),\n",
       "             ('[unused594]', 599),\n",
       "             ('[unused595]', 600),\n",
       "             ('[unused596]', 601),\n",
       "             ('[unused597]', 602),\n",
       "             ('[unused598]', 603),\n",
       "             ('[unused599]', 604),\n",
       "             ('[unused600]', 605),\n",
       "             ('[unused601]', 606),\n",
       "             ('[unused602]', 607),\n",
       "             ('[unused603]', 608),\n",
       "             ('[unused604]', 609),\n",
       "             ('[unused605]', 610),\n",
       "             ('[unused606]', 611),\n",
       "             ('[unused607]', 612),\n",
       "             ('[unused608]', 613),\n",
       "             ('[unused609]', 614),\n",
       "             ('[unused610]', 615),\n",
       "             ('[unused611]', 616),\n",
       "             ('[unused612]', 617),\n",
       "             ('[unused613]', 618),\n",
       "             ('[unused614]', 619),\n",
       "             ('[unused615]', 620),\n",
       "             ('[unused616]', 621),\n",
       "             ('[unused617]', 622),\n",
       "             ('[unused618]', 623),\n",
       "             ('[unused619]', 624),\n",
       "             ('[unused620]', 625),\n",
       "             ('[unused621]', 626),\n",
       "             ('[unused622]', 627),\n",
       "             ('[unused623]', 628),\n",
       "             ('[unused624]', 629),\n",
       "             ('[unused625]', 630),\n",
       "             ('[unused626]', 631),\n",
       "             ('[unused627]', 632),\n",
       "             ('[unused628]', 633),\n",
       "             ('[unused629]', 634),\n",
       "             ('[unused630]', 635),\n",
       "             ('[unused631]', 636),\n",
       "             ('[unused632]', 637),\n",
       "             ('[unused633]', 638),\n",
       "             ('[unused634]', 639),\n",
       "             ('[unused635]', 640),\n",
       "             ('[unused636]', 641),\n",
       "             ('[unused637]', 642),\n",
       "             ('[unused638]', 643),\n",
       "             ('[unused639]', 644),\n",
       "             ('[unused640]', 645),\n",
       "             ('[unused641]', 646),\n",
       "             ('[unused642]', 647),\n",
       "             ('[unused643]', 648),\n",
       "             ('[unused644]', 649),\n",
       "             ('[unused645]', 650),\n",
       "             ('[unused646]', 651),\n",
       "             ('[unused647]', 652),\n",
       "             ('[unused648]', 653),\n",
       "             ('[unused649]', 654),\n",
       "             ('[unused650]', 655),\n",
       "             ('[unused651]', 656),\n",
       "             ('[unused652]', 657),\n",
       "             ('[unused653]', 658),\n",
       "             ('[unused654]', 659),\n",
       "             ('[unused655]', 660),\n",
       "             ('[unused656]', 661),\n",
       "             ('[unused657]', 662),\n",
       "             ('[unused658]', 663),\n",
       "             ('[unused659]', 664),\n",
       "             ('[unused660]', 665),\n",
       "             ('[unused661]', 666),\n",
       "             ('[unused662]', 667),\n",
       "             ('[unused663]', 668),\n",
       "             ('[unused664]', 669),\n",
       "             ('[unused665]', 670),\n",
       "             ('[unused666]', 671),\n",
       "             ('[unused667]', 672),\n",
       "             ('[unused668]', 673),\n",
       "             ('[unused669]', 674),\n",
       "             ('[unused670]', 675),\n",
       "             ('[unused671]', 676),\n",
       "             ('[unused672]', 677),\n",
       "             ('[unused673]', 678),\n",
       "             ('[unused674]', 679),\n",
       "             ('[unused675]', 680),\n",
       "             ('[unused676]', 681),\n",
       "             ('[unused677]', 682),\n",
       "             ('[unused678]', 683),\n",
       "             ('[unused679]', 684),\n",
       "             ('[unused680]', 685),\n",
       "             ('[unused681]', 686),\n",
       "             ('[unused682]', 687),\n",
       "             ('[unused683]', 688),\n",
       "             ('[unused684]', 689),\n",
       "             ('[unused685]', 690),\n",
       "             ('[unused686]', 691),\n",
       "             ('[unused687]', 692),\n",
       "             ('[unused688]', 693),\n",
       "             ('[unused689]', 694),\n",
       "             ('[unused690]', 695),\n",
       "             ('[unused691]', 696),\n",
       "             ('[unused692]', 697),\n",
       "             ('[unused693]', 698),\n",
       "             ('[unused694]', 699),\n",
       "             ('[unused695]', 700),\n",
       "             ('[unused696]', 701),\n",
       "             ('[unused697]', 702),\n",
       "             ('[unused698]', 703),\n",
       "             ('[unused699]', 704),\n",
       "             ('[unused700]', 705),\n",
       "             ('[unused701]', 706),\n",
       "             ('[unused702]', 707),\n",
       "             ('[unused703]', 708),\n",
       "             ('[unused704]', 709),\n",
       "             ('[unused705]', 710),\n",
       "             ('[unused706]', 711),\n",
       "             ('[unused707]', 712),\n",
       "             ('[unused708]', 713),\n",
       "             ('[unused709]', 714),\n",
       "             ('[unused710]', 715),\n",
       "             ('[unused711]', 716),\n",
       "             ('[unused712]', 717),\n",
       "             ('[unused713]', 718),\n",
       "             ('[unused714]', 719),\n",
       "             ('[unused715]', 720),\n",
       "             ('[unused716]', 721),\n",
       "             ('[unused717]', 722),\n",
       "             ('[unused718]', 723),\n",
       "             ('[unused719]', 724),\n",
       "             ('[unused720]', 725),\n",
       "             ('[unused721]', 726),\n",
       "             ('[unused722]', 727),\n",
       "             ('[unused723]', 728),\n",
       "             ('[unused724]', 729),\n",
       "             ('[unused725]', 730),\n",
       "             ('[unused726]', 731),\n",
       "             ('[unused727]', 732),\n",
       "             ('[unused728]', 733),\n",
       "             ('[unused729]', 734),\n",
       "             ('[unused730]', 735),\n",
       "             ('[unused731]', 736),\n",
       "             ('[unused732]', 737),\n",
       "             ('[unused733]', 738),\n",
       "             ('[unused734]', 739),\n",
       "             ('[unused735]', 740),\n",
       "             ('[unused736]', 741),\n",
       "             ('[unused737]', 742),\n",
       "             ('[unused738]', 743),\n",
       "             ('[unused739]', 744),\n",
       "             ('[unused740]', 745),\n",
       "             ('[unused741]', 746),\n",
       "             ('[unused742]', 747),\n",
       "             ('[unused743]', 748),\n",
       "             ('[unused744]', 749),\n",
       "             ('[unused745]', 750),\n",
       "             ('[unused746]', 751),\n",
       "             ('[unused747]', 752),\n",
       "             ('[unused748]', 753),\n",
       "             ('[unused749]', 754),\n",
       "             ('[unused750]', 755),\n",
       "             ('[unused751]', 756),\n",
       "             ('[unused752]', 757),\n",
       "             ('[unused753]', 758),\n",
       "             ('[unused754]', 759),\n",
       "             ('[unused755]', 760),\n",
       "             ('[unused756]', 761),\n",
       "             ('[unused757]', 762),\n",
       "             ('[unused758]', 763),\n",
       "             ('[unused759]', 764),\n",
       "             ('[unused760]', 765),\n",
       "             ('[unused761]', 766),\n",
       "             ('[unused762]', 767),\n",
       "             ('[unused763]', 768),\n",
       "             ('[unused764]', 769),\n",
       "             ('[unused765]', 770),\n",
       "             ('[unused766]', 771),\n",
       "             ('[unused767]', 772),\n",
       "             ('[unused768]', 773),\n",
       "             ('[unused769]', 774),\n",
       "             ('[unused770]', 775),\n",
       "             ('[unused771]', 776),\n",
       "             ('[unused772]', 777),\n",
       "             ('[unused773]', 778),\n",
       "             ('[unused774]', 779),\n",
       "             ('[unused775]', 780),\n",
       "             ('[unused776]', 781),\n",
       "             ('[unused777]', 782),\n",
       "             ('[unused778]', 783),\n",
       "             ('[unused779]', 784),\n",
       "             ('[unused780]', 785),\n",
       "             ('[unused781]', 786),\n",
       "             ('[unused782]', 787),\n",
       "             ('[unused783]', 788),\n",
       "             ('[unused784]', 789),\n",
       "             ('[unused785]', 790),\n",
       "             ('[unused786]', 791),\n",
       "             ('[unused787]', 792),\n",
       "             ('[unused788]', 793),\n",
       "             ('[unused789]', 794),\n",
       "             ('[unused790]', 795),\n",
       "             ('[unused791]', 796),\n",
       "             ('[unused792]', 797),\n",
       "             ('[unused793]', 798),\n",
       "             ('[unused794]', 799),\n",
       "             ('[unused795]', 800),\n",
       "             ('[unused796]', 801),\n",
       "             ('[unused797]', 802),\n",
       "             ('[unused798]', 803),\n",
       "             ('[unused799]', 804),\n",
       "             ('[unused800]', 805),\n",
       "             ('[unused801]', 806),\n",
       "             ('[unused802]', 807),\n",
       "             ('[unused803]', 808),\n",
       "             ('[unused804]', 809),\n",
       "             ('[unused805]', 810),\n",
       "             ('[unused806]', 811),\n",
       "             ('[unused807]', 812),\n",
       "             ('[unused808]', 813),\n",
       "             ('[unused809]', 814),\n",
       "             ('[unused810]', 815),\n",
       "             ('[unused811]', 816),\n",
       "             ('[unused812]', 817),\n",
       "             ('[unused813]', 818),\n",
       "             ('[unused814]', 819),\n",
       "             ('[unused815]', 820),\n",
       "             ('[unused816]', 821),\n",
       "             ('[unused817]', 822),\n",
       "             ('[unused818]', 823),\n",
       "             ('[unused819]', 824),\n",
       "             ('[unused820]', 825),\n",
       "             ('[unused821]', 826),\n",
       "             ('[unused822]', 827),\n",
       "             ('[unused823]', 828),\n",
       "             ('[unused824]', 829),\n",
       "             ('[unused825]', 830),\n",
       "             ('[unused826]', 831),\n",
       "             ('[unused827]', 832),\n",
       "             ('[unused828]', 833),\n",
       "             ('[unused829]', 834),\n",
       "             ('[unused830]', 835),\n",
       "             ('[unused831]', 836),\n",
       "             ('[unused832]', 837),\n",
       "             ('[unused833]', 838),\n",
       "             ('[unused834]', 839),\n",
       "             ('[unused835]', 840),\n",
       "             ('[unused836]', 841),\n",
       "             ('[unused837]', 842),\n",
       "             ('[unused838]', 843),\n",
       "             ('[unused839]', 844),\n",
       "             ('[unused840]', 845),\n",
       "             ('[unused841]', 846),\n",
       "             ('[unused842]', 847),\n",
       "             ('[unused843]', 848),\n",
       "             ('[unused844]', 849),\n",
       "             ('[unused845]', 850),\n",
       "             ('[unused846]', 851),\n",
       "             ('[unused847]', 852),\n",
       "             ('[unused848]', 853),\n",
       "             ('[unused849]', 854),\n",
       "             ('[unused850]', 855),\n",
       "             ('[unused851]', 856),\n",
       "             ('[unused852]', 857),\n",
       "             ('[unused853]', 858),\n",
       "             ('[unused854]', 859),\n",
       "             ('[unused855]', 860),\n",
       "             ('[unused856]', 861),\n",
       "             ('[unused857]', 862),\n",
       "             ('[unused858]', 863),\n",
       "             ('[unused859]', 864),\n",
       "             ('[unused860]', 865),\n",
       "             ('[unused861]', 866),\n",
       "             ('[unused862]', 867),\n",
       "             ('[unused863]', 868),\n",
       "             ('[unused864]', 869),\n",
       "             ('[unused865]', 870),\n",
       "             ('[unused866]', 871),\n",
       "             ('[unused867]', 872),\n",
       "             ('[unused868]', 873),\n",
       "             ('[unused869]', 874),\n",
       "             ('[unused870]', 875),\n",
       "             ('[unused871]', 876),\n",
       "             ('[unused872]', 877),\n",
       "             ('[unused873]', 878),\n",
       "             ('[unused874]', 879),\n",
       "             ('[unused875]', 880),\n",
       "             ('[unused876]', 881),\n",
       "             ('[unused877]', 882),\n",
       "             ('[unused878]', 883),\n",
       "             ('[unused879]', 884),\n",
       "             ('[unused880]', 885),\n",
       "             ('[unused881]', 886),\n",
       "             ('[unused882]', 887),\n",
       "             ('[unused883]', 888),\n",
       "             ('[unused884]', 889),\n",
       "             ('[unused885]', 890),\n",
       "             ('[unused886]', 891),\n",
       "             ('[unused887]', 892),\n",
       "             ('[unused888]', 893),\n",
       "             ('[unused889]', 894),\n",
       "             ('[unused890]', 895),\n",
       "             ('[unused891]', 896),\n",
       "             ('[unused892]', 897),\n",
       "             ('[unused893]', 898),\n",
       "             ('[unused894]', 899),\n",
       "             ('[unused895]', 900),\n",
       "             ('[unused896]', 901),\n",
       "             ('[unused897]', 902),\n",
       "             ('[unused898]', 903),\n",
       "             ('[unused899]', 904),\n",
       "             ('[unused900]', 905),\n",
       "             ('[unused901]', 906),\n",
       "             ('[unused902]', 907),\n",
       "             ('[unused903]', 908),\n",
       "             ('[unused904]', 909),\n",
       "             ('[unused905]', 910),\n",
       "             ('[unused906]', 911),\n",
       "             ('[unused907]', 912),\n",
       "             ('[unused908]', 913),\n",
       "             ('[unused909]', 914),\n",
       "             ('[unused910]', 915),\n",
       "             ('[unused911]', 916),\n",
       "             ('[unused912]', 917),\n",
       "             ('[unused913]', 918),\n",
       "             ('[unused914]', 919),\n",
       "             ('[unused915]', 920),\n",
       "             ('[unused916]', 921),\n",
       "             ('[unused917]', 922),\n",
       "             ('[unused918]', 923),\n",
       "             ('[unused919]', 924),\n",
       "             ('[unused920]', 925),\n",
       "             ('[unused921]', 926),\n",
       "             ('[unused922]', 927),\n",
       "             ('[unused923]', 928),\n",
       "             ('[unused924]', 929),\n",
       "             ('[unused925]', 930),\n",
       "             ('[unused926]', 931),\n",
       "             ('[unused927]', 932),\n",
       "             ('[unused928]', 933),\n",
       "             ('[unused929]', 934),\n",
       "             ('[unused930]', 935),\n",
       "             ('[unused931]', 936),\n",
       "             ('[unused932]', 937),\n",
       "             ('[unused933]', 938),\n",
       "             ('[unused934]', 939),\n",
       "             ('[unused935]', 940),\n",
       "             ('[unused936]', 941),\n",
       "             ('[unused937]', 942),\n",
       "             ('[unused938]', 943),\n",
       "             ('[unused939]', 944),\n",
       "             ('[unused940]', 945),\n",
       "             ('[unused941]', 946),\n",
       "             ('[unused942]', 947),\n",
       "             ('[unused943]', 948),\n",
       "             ('[unused944]', 949),\n",
       "             ('[unused945]', 950),\n",
       "             ('[unused946]', 951),\n",
       "             ('[unused947]', 952),\n",
       "             ('[unused948]', 953),\n",
       "             ('[unused949]', 954),\n",
       "             ('[unused950]', 955),\n",
       "             ('[unused951]', 956),\n",
       "             ('[unused952]', 957),\n",
       "             ('[unused953]', 958),\n",
       "             ('[unused954]', 959),\n",
       "             ('[unused955]', 960),\n",
       "             ('[unused956]', 961),\n",
       "             ('[unused957]', 962),\n",
       "             ('[unused958]', 963),\n",
       "             ('[unused959]', 964),\n",
       "             ('[unused960]', 965),\n",
       "             ('[unused961]', 966),\n",
       "             ('[unused962]', 967),\n",
       "             ('[unused963]', 968),\n",
       "             ('[unused964]', 969),\n",
       "             ('[unused965]', 970),\n",
       "             ('[unused966]', 971),\n",
       "             ('[unused967]', 972),\n",
       "             ('[unused968]', 973),\n",
       "             ('[unused969]', 974),\n",
       "             ('[unused970]', 975),\n",
       "             ('[unused971]', 976),\n",
       "             ('[unused972]', 977),\n",
       "             ('[unused973]', 978),\n",
       "             ('[unused974]', 979),\n",
       "             ('[unused975]', 980),\n",
       "             ('[unused976]', 981),\n",
       "             ('[unused977]', 982),\n",
       "             ('[unused978]', 983),\n",
       "             ('[unused979]', 984),\n",
       "             ('[unused980]', 985),\n",
       "             ('[unused981]', 986),\n",
       "             ('[unused982]', 987),\n",
       "             ('[unused983]', 988),\n",
       "             ('[unused984]', 989),\n",
       "             ('[unused985]', 990),\n",
       "             ('[unused986]', 991),\n",
       "             ('[unused987]', 992),\n",
       "             ('[unused988]', 993),\n",
       "             ('[unused989]', 994),\n",
       "             ('[unused990]', 995),\n",
       "             ('[unused991]', 996),\n",
       "             ('[unused992]', 997),\n",
       "             ('[unused993]', 998),\n",
       "             ('!', 999),\n",
       "             ...])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import BasicTokenizer, WordpieceTokenizer\n",
    "\n",
    "# BasicTokenizer, WordpieceTokenizerは、引用文献[2]そのまま\n",
    "# https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/tokenization.py\n",
    "# これらはsub-wordで単語分割を行うクラスになる\n",
    "\n",
    "\n",
    "class BertTokenizer(object):\n",
    "    '''BERT用の文章の単語分割クラスを実装'''\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        '''\n",
    "        vocab_file：ボキャブラリーへのパス\n",
    "        do_lower_case：前処理で単語を小文字化するかどうか\n",
    "        '''\n",
    "\n",
    "        # ボキャブラリーのロード\n",
    "        self.vocab, self.ids_to_tokens = load_vocab(vocab_file)\n",
    "\n",
    "        # 分割処理の関数をフォルダ「utils」からimoprt、sub-wordで単語分割を行う\n",
    "        never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
    "        # (注釈)上記の単語は途中で分割させない．これで一つの単語とみなす\n",
    "\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
    "                                              never_split=never_split)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        '''文章を単語に分割する関数'''\n",
    "        split_tokens = []  # 分割後の単語群\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"分割された単語リストをIDに変換する関数\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab[token])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"IDを単語に変換する関数\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.ids_to_tokens[i])\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[CLS]', 'i', 'accessed', 'the', 'bank', 'account', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 文章1：銀行口座にアクセスしました．\n",
    "text_1 = \"[CLS] I accessed the bank account. [SEP]\"\n",
    "\n",
    "# 文章2：彼は敷金を銀行口座に振り込みました．\n",
    "text_2 = \"[CLS] He transferred the deposit money into the bank account. [SEP]\"\n",
    "\n",
    "# 文章3：川岸でサッカーをします．\n",
    "text_3 = \"[CLS] We play soccer at the bank of the river. [SEP]\"\n",
    "\n",
    "# 単語分割Tokenizerを用意\n",
    "tokenizer = BertTokenizer(\n",
    "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\", do_lower_case=True)\n",
    "\n",
    "# 文章を単語分割\n",
    "tokenized_text_1 = tokenizer.tokenize(text_1)\n",
    "tokenized_text_2 = tokenizer.tokenize(text_2)\n",
    "tokenized_text_3 = tokenizer.tokenize(text_3)\n",
    "\n",
    "# 確認\n",
    "print(tokenized_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[  101,  1045, 11570,  1996,  2924,  4070,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "# 単語をIDに変換する\n",
    "indexed_tokens_1 = tokenizer.convert_tokens_to_ids(tokenized_text_1)\n",
    "indexed_tokens_2 = tokenizer.convert_tokens_to_ids(tokenized_text_2)\n",
    "indexed_tokens_3 = tokenizer.convert_tokens_to_ids(tokenized_text_3)\n",
    "\n",
    "# 各文章のbankの位置\n",
    "bank_posi_1 = np.where(np.array(tokenized_text_1) == \"bank\")[0][0]  # 4\n",
    "bank_posi_2 = np.where(np.array(tokenized_text_2) == \"bank\")[0][0]  # 8\n",
    "bank_posi_3 = np.where(np.array(tokenized_text_3) == \"bank\")[0][0]  # 6\n",
    "\n",
    "# seqId（1文目か2文目かは今回は必要ない）\n",
    "\n",
    "# リストをPyTorchのテンソルに\n",
    "tokens_tensor_1 = torch.tensor([indexed_tokens_1])\n",
    "tokens_tensor_2 = torch.tensor([indexed_tokens_2])\n",
    "tokens_tensor_3 = torch.tensor([indexed_tokens_3])\n",
    "\n",
    "# bankの単語id\n",
    "bank_word_id = tokenizer.convert_tokens_to_ids([\"bank\"])[0]\n",
    "\n",
    "# 確認\n",
    "print(tokens_tensor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章をBERTで処理\n",
    "with torch.no_grad():\n",
    "    encoded_layers_1, _ = net(tokens_tensor_1, output_all_encoded_layers=True)\n",
    "    encoded_layers_2, _ = net(tokens_tensor_2, output_all_encoded_layers=True)\n",
    "    encoded_layers_3, _ = net(tokens_tensor_3, output_all_encoded_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bankの初期の単語ベクトル表現\n",
    "# これはEmbeddingsモジュールから取り出し，単語bankのidに応じた単語ベクトルなので3文で共通している\n",
    "bank_vector_0 = net.embeddings.word_embeddings.weight[bank_word_id]\n",
    "\n",
    "# 文章1のBertLayerモジュール1段目から出力されるbankの特徴量ベクトル\n",
    "bank_vector_1_1 = encoded_layers_1[0][0, bank_posi_1]\n",
    "\n",
    "# 文章1のBertLayerモジュール最終12段目から出力されるのbankの特徴量ベクトル\n",
    "bank_vector_1_12 = encoded_layers_1[11][0, bank_posi_1]\n",
    "\n",
    "# 文章2，3も同様に\n",
    "bank_vector_2_1 = encoded_layers_2[0][0, bank_posi_2]\n",
    "bank_vector_2_12 = encoded_layers_2[11][0, bank_posi_2]\n",
    "bank_vector_3_1 = encoded_layers_3[0][0, bank_posi_3]\n",
    "bank_vector_3_12 = encoded_layers_3[11][0, bank_posi_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bankの初期ベクトル と 文章1の1段目のbankの類似度： tensor(0.6814, grad_fn=<DivBackward0>)\nbankの初期ベクトル と 文章1の12段目のbankの類似度： tensor(0.2276, grad_fn=<DivBackward0>)\n文章1の1層目のbank と 文章2の1段目のbankの類似度： tensor(0.8968)\n文章1の1層目のbank と 文章3の1段目のbankの類似度： tensor(0.7584)\n文章1の12層目のbank と 文章2の12段目のbankの類似度： tensor(0.8796)\n文章1の12層目のbank と 文章3の12段目のbankの類似度： tensor(0.4814)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"bankの初期ベクトル と 文章1の1段目のbankの類似度：\",\n",
    "      F.cosine_similarity(bank_vector_0, bank_vector_1_1, dim=0))\n",
    "print(\"bankの初期ベクトル と 文章1の12段目のbankの類似度：\",\n",
    "      F.cosine_similarity(bank_vector_0, bank_vector_1_12, dim=0))\n",
    "\n",
    "print(\"文章1の1層目のbank と 文章2の1段目のbankの類似度：\",\n",
    "      F.cosine_similarity(bank_vector_1_1, bank_vector_2_1, dim=0))\n",
    "print(\"文章1の1層目のbank と 文章3の1段目のbankの類似度：\",\n",
    "      F.cosine_similarity(bank_vector_1_1, bank_vector_3_1, dim=0))\n",
    "\n",
    "print(\"文章1の12層目のbank と 文章2の12段目のbankの類似度：\",\n",
    "      F.cosine_similarity(bank_vector_1_12, bank_vector_2_12, dim=0))\n",
    "print(\"文章1の12層目のbank と 文章3の12段目のbankの類似度：\",\n",
    "      F.cosine_similarity(bank_vector_1_12, bank_vector_3_12, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}